{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded successfully\n"
     ]
    }
   ],
   "source": [
    "# Attribution path: \"{image_index}.{layer}.{attribution}.{upsample}.png\"\n",
    "# Image path: \"{image_index}.png\"\n",
    "# Attribution metrics: \"{image_index}.{layer}.{attribution}.{upsample}.csv\"\n",
    "\n",
    "from utils import _DeepLiftShap, _GradCAMPlusPlus, ERFUpsampling, SimpleUpsampling, ERFUpsamplingFast\n",
    "from data import PascalVOC2007\n",
    "from models import vgg11_PascalVOC, vgg_preprocess\n",
    "import torch\n",
    "\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "model = vgg11_PascalVOC()\n",
    "model.to(device)\n",
    "# Load the pretrained weights\n",
    "model.load_state_dict(torch.load(\"VGG11_PascalVOC.pt\", map_location=device))\n",
    "model.eval()\n",
    "\n",
    "print(\"Model loaded successfully\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using downloaded and verified file: data\\VOCtest_06-Nov-2007.tar\n",
      "Extracting data\\VOCtest_06-Nov-2007.tar to data\n",
      "Using downloaded and verified file: data\\VOCtrainval_06-Nov-2007.tar\n",
      "Extracting data\\VOCtrainval_06-Nov-2007.tar to data\n"
     ]
    }
   ],
   "source": [
    "test_data = PascalVOC2007(\"test\", transform=vgg_preprocess)\n",
    "train_data = PascalVOC2007(\"train\", transform=vgg_preprocess)\n",
    "\n",
    "baseline_dist = baseline_dist = torch.cat([train_data[i][0].unsqueeze(0) for i in range(16)], dim=0).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "def save_image(image: torch.Tensor, path: str) -> None:\n",
    "    # Assume shape is (C, H, W)\n",
    "    if len(image.shape) != 3:\n",
    "        raise ValueError(\"Image shape should be (C, H, W)\")\n",
    "    \n",
    "    image = image.permute(1, 2, 0).detach().cpu().numpy()\n",
    "    image = (image - image.min()) / (image.max() - image.min())\n",
    "    image = (image * 255).astype(\"uint8\")\n",
    "    \n",
    "    # Convert from RGB to BGR\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR)\n",
    "\n",
    "    cv2.imwrite(path, image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 224, 224]) torch.Size([1])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Luca\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\captum\\attr\\_core\\deep_lift.py:304: UserWarning: Setting forward, backward hooks and attributes on non-linear\n",
      "               activations. The hooks and attributes will be removed\n",
      "            after the attribution is finished\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 1, 224, 224])\n",
      "torch.Size([1, 1, 224, 224])\n",
      "torch.Size([1, 1, 224, 224])\n",
      "torch.Size([1, 1, 224, 224])\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "ROOT_PATH = \".\"\n",
    "OUTPUT_PATH = os.path.join(ROOT_PATH, \"output\")\n",
    "\n",
    "IMAGES_PATH = os.path.join(OUTPUT_PATH, \"images\")\n",
    "ATTRIBUTIONS_PATH = os.path.join(OUTPUT_PATH, \"attributions\")\n",
    "\n",
    "for dirs in [OUTPUT_PATH, IMAGES_PATH, ATTRIBUTIONS_PATH]:\n",
    "    if not os.path.exists(dirs):\n",
    "        os.makedirs(dirs)\n",
    "\n",
    "\n",
    "for image_index in range(1):\n",
    "\n",
    "    image, label = test_data[image_index]\n",
    "    images = image.unsqueeze(0).to(device)\n",
    "    labels = label.to(device)\n",
    "    print(image.shape, label.shape)\n",
    "\n",
    "\n",
    "    image_path = os.path.join(IMAGES_PATH, f\"{image_index}.png\")\n",
    "    # Save image to disk\n",
    "    save_image(image, image_path)\n",
    "\n",
    "    \n",
    "    for layer_index in range(len(model.features)-1, len(model.features)):\n",
    "        layer = model.features[layer_index]\n",
    "        for attribution in [_DeepLiftShap(), _GradCAMPlusPlus(model, layer)]:\n",
    "            attribution_name = attribution.__class__.__name__\n",
    "            for upsample in [SimpleUpsampling((224,224)), ERFUpsamplingFast(model, layer, device)]:\n",
    "                upsample_name = upsample.__class__.__name__\n",
    "                attribution_path = os.path.join(ATTRIBUTIONS_PATH, f\"{image_index}.{layer_index}.{attribution_name}.{upsample_name}.png\")\n",
    "\n",
    "                attribution_map = attribution.attribute(\n",
    "                    model=model,\n",
    "                    input_tensor=images,\n",
    "                    layer=layer,\n",
    "                    target=labels,\n",
    "                    baseline_dist=baseline_dist,\n",
    "                )\n",
    "\n",
    "                attribution_map = upsample(attribution_map, images)\n",
    "                save_image(attribution_map[0], attribution_path)\n",
    "\n",
    "                metrics_path = os.path.join(ATTRIBUTIONS_PATH, f\"{image_index}.{layer_index}.{attribution_name}.{upsample_name}.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_images(image_index:int, layer_index:int, attribution_name: str, upsample_name: str):\n",
    "    # Calculate the attribution maps and return it\n",
    "    image, label = test_data[image_index]\n",
    "    images = image.unsqueeze(0).to(device)\n",
    "\n",
    "    layer = model.features[layer_index]\n",
    "\n",
    "    if attribution_name == \"_DeepLiftShap\":\n",
    "        attribution = _DeepLiftShap()\n",
    "    elif attribution_name == \"_GradCAMPlusPlus\":\n",
    "        attribution = _GradCAMPlusPlus(model, layer)\n",
    "    else:\n",
    "        raise ValueError(f\"Invalid attribution name, got {attribution_name}\")\n",
    "    \n",
    "    if upsample_name == \"SimpleUpsampling\":\n",
    "        upsample = SimpleUpsampling((224,224))\n",
    "    elif upsample_name == \"ERFUpsampling\":\n",
    "        upsample = ERFUpsampling(model, layer, device)\n",
    "    elif upsample_name == \"ERFUpsamplingFast\":\n",
    "        upsample = ERFUpsamplingFast(model, layer, device)\n",
    "    else:\n",
    "        raise ValueError(f\"Invalid upsample name, got {upsample_name}\")\n",
    "    \n",
    "    attribution_map = attribution.attribute(\n",
    "        model=model,\n",
    "        input_tensor=images,\n",
    "        layer=layer,\n",
    "        target=label.to(device),\n",
    "        baseline_dist=baseline_dist,\n",
    "    )\n",
    "\n",
    "    attribution_map = upsample(attribution_map, images)\n",
    "\n",
    "    return attribution_map\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
