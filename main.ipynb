{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\Luca\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "\n",
    "%autoreload 2\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import transforms\n",
    "\n",
    "from utils import _DeepLiftShap, _GradCAMPlusPlus, SimpleUpsampling, ERFUpsampling,ERFUpsamplingFast, cut_model_from_layer, cut_model_to_layer\n",
    "from data import PascalVOC2007\n",
    "from results.results_metrics import ResultMetrics\n",
    "from models import vgg11_PascalVOC\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    \n",
    "torch.manual_seed(123)\n",
    "np.random.seed(123)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here the VGG-11 model is instantiated, the last layer is replaced with a new layer that has 20 outputs instead of 1000 (because PascalVOC has 20 classes, but the model was trained on ImageNet which has 1000 classes). The loaded weights are pretrained on ImageNet and the fine-tuned on PascalVOC. For more detail about the fine-tuning process, see `fine_tuning.py`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = vgg11_PascalVOC()\n",
    "model.to(device)\n",
    "# Load the pretrained weights\n",
    "model.load_state_dict(torch.load('VGG11_PascalVOC.pt', map_location=device))\n",
    "model.eval()\n",
    "\n",
    "preprocess = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),                 # Convert to Tensor\n",
    "    transforms.Normalize(                  # Normalize using ImageNet mean and std\n",
    "        mean=[0.485, 0.456, 0.406],\n",
    "        std=[0.229, 0.224, 0.225]\n",
    "    ),\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VGG(\n",
       "  (features): Sequential(\n",
       "    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (1): ReLU(inplace=True)\n",
       "    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (3): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (4): ReLU(inplace=True)\n",
       "    (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (6): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (7): ReLU(inplace=True)\n",
       "    (8): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (9): ReLU(inplace=True)\n",
       "    (10): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (11): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (12): ReLU(inplace=True)\n",
       "    (13): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (14): ReLU(inplace=True)\n",
       "    (15): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (16): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (17): ReLU(inplace=True)\n",
       "    (18): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (19): ReLU(inplace=True)\n",
       "    (20): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  )\n",
       "  (avgpool): AdaptiveAvgPool2d(output_size=(7, 7))\n",
       "  (classifier): Sequential(\n",
       "    (0): Linear(in_features=25088, out_features=4096, bias=True)\n",
       "    (1): ReLU(inplace=True)\n",
       "    (2): Dropout(p=0.5, inplace=False)\n",
       "    (3): Linear(in_features=4096, out_features=4096, bias=True)\n",
       "    (4): ReLU(inplace=True)\n",
       "    (5): Dropout(p=0.5, inplace=False)\n",
       "    (6): Linear(in_features=4096, out_features=20, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1         [-1, 64, 224, 224]           1,792\n",
      "              ReLU-2         [-1, 64, 224, 224]               0\n",
      "         MaxPool2d-3         [-1, 64, 112, 112]               0\n",
      "            Conv2d-4        [-1, 128, 112, 112]          73,856\n",
      "              ReLU-5        [-1, 128, 112, 112]               0\n",
      "         MaxPool2d-6          [-1, 128, 56, 56]               0\n",
      "            Conv2d-7          [-1, 256, 56, 56]         295,168\n",
      "              ReLU-8          [-1, 256, 56, 56]               0\n",
      "            Conv2d-9          [-1, 256, 56, 56]         590,080\n",
      "             ReLU-10          [-1, 256, 56, 56]               0\n",
      "        MaxPool2d-11          [-1, 256, 28, 28]               0\n",
      "           Conv2d-12          [-1, 512, 28, 28]       1,180,160\n",
      "             ReLU-13          [-1, 512, 28, 28]               0\n",
      "           Conv2d-14          [-1, 512, 28, 28]       2,359,808\n",
      "             ReLU-15          [-1, 512, 28, 28]               0\n",
      "        MaxPool2d-16          [-1, 512, 14, 14]               0\n",
      "           Conv2d-17          [-1, 512, 14, 14]       2,359,808\n",
      "             ReLU-18          [-1, 512, 14, 14]               0\n",
      "           Conv2d-19          [-1, 512, 14, 14]       2,359,808\n",
      "             ReLU-20          [-1, 512, 14, 14]               0\n",
      "        MaxPool2d-21            [-1, 512, 7, 7]               0\n",
      "AdaptiveAvgPool2d-22            [-1, 512, 7, 7]               0\n",
      "           Linear-23                 [-1, 4096]     102,764,544\n",
      "             ReLU-24                 [-1, 4096]               0\n",
      "          Dropout-25                 [-1, 4096]               0\n",
      "           Linear-26                 [-1, 4096]      16,781,312\n",
      "             ReLU-27                 [-1, 4096]               0\n",
      "          Dropout-28                 [-1, 4096]               0\n",
      "           Linear-29                   [-1, 20]          81,940\n",
      "================================================================\n",
      "Total params: 128,848,276\n",
      "Trainable params: 128,848,276\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.57\n",
      "Forward/backward pass size (MB): 125.37\n",
      "Params size (MB): 491.52\n",
      "Estimated Total Size (MB): 617.46\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "import torchsummary\n",
    "\n",
    "torchsummary.summary(model, (3, 224, 224))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using downloaded and verified file: data\\VOCtest_06-Nov-2007.tar\n",
      "Extracting data\\VOCtest_06-Nov-2007.tar to data\n",
      "Using downloaded and verified file: data\\VOCtrainval_06-Nov-2007.tar\n",
      "Extracting data\\VOCtrainval_06-Nov-2007.tar to data\n"
     ]
    }
   ],
   "source": [
    "test_data = PascalVOC2007(\"test\", transform=preprocess)\n",
    "train_data = PascalVOC2007(\"trainval\", transform=preprocess)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The train data is only used in the case of DeepLiftSHAP, because it requires a baseline distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.utils\n",
    "from torch.utils.data import Subset\n",
    "\n",
    "BATCH_SIZE_TEST = 1\n",
    "NUM_TEST = 128\n",
    "\n",
    "BATCH_SIZE_TRAIN = 2\n",
    "NUM_TRAIN = 8\n",
    "\n",
    "dl_test = DataLoader(Subset(test_data, torch.randperm(len(test_data))[:NUM_TEST]), batch_size=BATCH_SIZE_TEST, shuffle=False)\n",
    "dl_train = DataLoader(Subset(train_data, torch.randperm(len(train_data))[:NUM_TRAIN]), batch_size=BATCH_SIZE_TRAIN, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The sum of all the attributions is equal to the difference between the output of the model on the current input, minus the average output of the model on the baseline distribution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The parameter `attribution_method` is an object that extends the base class `AttributionMethod` and implements the method `attribute`. In this way, simply changing the object passed to the `explain` method, we can change the attribution method used.\n",
    "- The parameter `train_dl` is only used by the DeepLiftSHAP method, because it requires a baseline distribution.\n",
    "- The function automatically add the metrics to the `ResultMetrics` object passed as parameter.\n",
    "- The boolean `rescale_saliency` is used to rescale the saliency map in such a way that the integral of the saliency map is equal to $r \\times (H \\times W)$, where $H$ and $W$ are the height and width of the saliency map, and $r \\in [0, 1]$ is the rescale factor.\n",
    "    - This is useful to compare different saliency maps methods, because, for example, producing a saliency map with all ones is not useful, but gives the best results for some of the metrics.\n",
    "    - The scaling is done in such a way that the positions and the values where the saliency map is equal to one are not changed. More formally, given the saliency map $S(i, j): \\mathbb{R}^2 \\rightarrow [0,1]$, the rescaled saliency map $S'(i, j) = S(i,j)^{\\alpha^2}$ is found as an optimization problem by defining:\n",
    "\n",
    "    $$L(S(i,j), \\alpha) = \\left( \\sum_{i,j} S(i,j)^{\\alpha^2}  - (r \\times H \\times W) \\right)^2$$\n",
    "\n",
    "    $$\\alpha = \\arg \\min_{\\alpha \\in \\mathbb{R}} L(S(i,j), \\alpha)$$\n",
    "\n",
    "    - It can be seen as a way to constraint the saliency map into keeping only a limited amount of important pixels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "bilinear_upsampling = SimpleUpsampling(size=(224, 224), mode='bilinear')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results loaded from ./results.csv.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "51d9562d1c5b4942bca2a2edcf59bce5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Luca\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\captum\\attr\\_core\\deep_lift.py:304: UserWarning: Setting forward, backward hooks and attributes on non-linear\n",
      "               activations. The hooks and attributes will be removed\n",
      "            after the attribution is finished\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "32f767e38ab3489bb7e403cfcce146da",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[19], line 8\u001b[0m\n\u001b[0;32m      5\u001b[0m results \u001b[38;5;241m=\u001b[39m ResultMetrics(RESULT_PATH)\n\u001b[0;32m      6\u001b[0m \u001b[38;5;66;03m# for f in model.features[4:]:\u001b[39;00m\n\u001b[0;32m      7\u001b[0m     \u001b[38;5;66;03m# attr = _GradCAMPlusPlus(model, f)\u001b[39;00m\n\u001b[1;32m----> 8\u001b[0m \u001b[43mcalculate_metrics\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      9\u001b[0m \u001b[43m                \u001b[49m\u001b[43m_DeepLiftShap\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     10\u001b[0m \u001b[43m                \u001b[49m\u001b[43mdl_test\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     11\u001b[0m \u001b[43m                \u001b[49m\u001b[43mdl_train\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     12\u001b[0m \u001b[43m                \u001b[49m\u001b[43mlayers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43mf\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mf\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfeatures\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     13\u001b[0m \u001b[43m                \u001b[49m\u001b[43mmetrics\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43mSensitivity\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mInfidelity\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mRoadCombined\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     14\u001b[0m \u001b[43m                \u001b[49m\u001b[43mresult_metrics\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresults\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     15\u001b[0m \u001b[43m                \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     16\u001b[0m \u001b[43m                \u001b[49m\u001b[43mupsample\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbilinear_upsampling\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     17\u001b[0m \u001b[43m                \u001b[49m\u001b[43mrescale_saliency\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m     18\u001b[0m \u001b[43m                \u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mVGG11\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     19\u001b[0m \u001b[43m                \u001b[49m\u001b[43mdebug\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\n\u001b[0;32m     20\u001b[0m \u001b[43m                \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Luca\\Desktop\\MagistraleAI\\2nd_year\\Artificial Intelligence in Industry\\project\\metrics\\metric.py:131\u001b[0m, in \u001b[0;36mcalculate_metrics\u001b[1;34m(model, attribute_method, test_dl, train_dl, layers, metrics, result_metrics, upsample, device, rescale_saliency, rescale_perc, model_name, debug)\u001b[0m\n\u001b[0;32m    129\u001b[0m \u001b[38;5;66;03m# Calculate all the metrics\u001b[39;00m\n\u001b[0;32m    130\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m metric \u001b[38;5;129;01min\u001b[39;00m metrics:\n\u001b[1;32m--> 131\u001b[0m     metric_res \u001b[38;5;241m=\u001b[39m \u001b[43mmetric\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    132\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    133\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtest_images\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mimages\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    134\u001b[0m \u001b[43m        \u001b[49m\u001b[43msaliency_maps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msaliency_maps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    135\u001b[0m \u001b[43m        \u001b[49m\u001b[43mclass_idx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlabels\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    136\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattribution_method\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattribute_method\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    137\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    138\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbaseline_dist\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbaseline_dist\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    139\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlayer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlayer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    140\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    142\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m metric_res \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    143\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMetric \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmetric\u001b[38;5;241m.\u001b[39mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m is None\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\Luca\\Desktop\\MagistraleAI\\2nd_year\\Artificial Intelligence in Industry\\project\\metrics\\sensitivity.py:77\u001b[0m, in \u001b[0;36mSensitivity.__call__\u001b[1;34m(self, model, test_images, saliency_maps, class_idx, attribution_method, device, apply_softmax, return_mean, **kwargs)\u001b[0m\n\u001b[0;32m     74\u001b[0m kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m model\n\u001b[0;32m     75\u001b[0m kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtargets\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m class_idx\n\u001b[1;32m---> 77\u001b[0m sens \u001b[38;5;241m=\u001b[39m sensitivity_max(attribution_wrapper, test_images, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m     79\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (sens \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m100\u001b[39m)\u001b[38;5;241m.\u001b[39many():\n\u001b[0;32m     80\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Luca\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\captum\\log\\__init__.py:42\u001b[0m, in \u001b[0;36mlog_usage.<locals>._log_usage.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     40\u001b[0m \u001b[38;5;129m@wraps\u001b[39m(func)\n\u001b[0;32m     41\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrapper\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m---> 42\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\Luca\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\captum\\metrics\\_core\\sensitivity.py:311\u001b[0m, in \u001b[0;36msensitivity_max\u001b[1;34m(explanation_func, inputs, perturb_func, perturb_radius, n_perturb_samples, norm_ord, max_examples_per_batch, **kwargs)\u001b[0m\n\u001b[0;32m    309\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m    310\u001b[0m     expl_inputs \u001b[38;5;241m=\u001b[39m explanation_func(inputs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m--> 311\u001b[0m     metrics_max \u001b[38;5;241m=\u001b[39m \u001b[43m_divide_and_aggregate_metrics\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    312\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcast\u001b[49m\u001b[43m(\u001b[49m\u001b[43mTuple\u001b[49m\u001b[43m[\u001b[49m\u001b[43mTensor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    313\u001b[0m \u001b[43m        \u001b[49m\u001b[43mn_perturb_samples\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    314\u001b[0m \u001b[43m        \u001b[49m\u001b[43m_next_sensitivity_max\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    315\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmax_examples_per_batch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_examples_per_batch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    316\u001b[0m \u001b[43m        \u001b[49m\u001b[43magg_func\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    317\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    318\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m metrics_max\n",
      "File \u001b[1;32mc:\\Users\\Luca\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\captum\\metrics\\_utils\\batching.py:71\u001b[0m, in \u001b[0;36m_divide_and_aggregate_metrics\u001b[1;34m(inputs, n_perturb_samples, metric_func, agg_func, max_examples_per_batch)\u001b[0m\n\u001b[0;32m     63\u001b[0m max_inps_per_batch \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m     64\u001b[0m     n_perturb_samples\n\u001b[0;32m     65\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m max_examples_per_batch \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m     66\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mmin\u001b[39m(\u001b[38;5;28mmax\u001b[39m(max_examples_per_batch \u001b[38;5;241m/\u001b[39m\u001b[38;5;241m/\u001b[39m bsz, \u001b[38;5;241m1\u001b[39m), n_perturb_samples)\n\u001b[0;32m     67\u001b[0m )\n\u001b[0;32m     69\u001b[0m current_n_steps \u001b[38;5;241m=\u001b[39m max_inps_per_batch\n\u001b[1;32m---> 71\u001b[0m metrics_sum \u001b[38;5;241m=\u001b[39m \u001b[43mmetric_func\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmax_inps_per_batch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     73\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m current_n_steps \u001b[38;5;241m<\u001b[39m n_perturb_samples:\n\u001b[0;32m     74\u001b[0m     current_n_steps \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m max_inps_per_batch\n",
      "File \u001b[1;32mc:\\Users\\Luca\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\captum\\metrics\\_core\\sensitivity.py:259\u001b[0m, in \u001b[0;36msensitivity_max.<locals>._next_sensitivity_max\u001b[1;34m(current_n_perturb_samples)\u001b[0m\n\u001b[0;32m    249\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[0;32m    250\u001b[0m             \u001b[38;5;28misinstance\u001b[39m(baselines[\u001b[38;5;241m0\u001b[39m], Tensor)\n\u001b[0;32m    251\u001b[0m             \u001b[38;5;129;01mand\u001b[39;00m baselines[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mshape \u001b[38;5;241m==\u001b[39m inputs[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mshape\n\u001b[0;32m    252\u001b[0m         ):\n\u001b[0;32m    253\u001b[0m             _expand_and_update_baselines(\n\u001b[0;32m    254\u001b[0m                 cast(Tuple[Tensor, \u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m], inputs),\n\u001b[0;32m    255\u001b[0m                 current_n_perturb_samples,\n\u001b[0;32m    256\u001b[0m                 kwargs_copy,\n\u001b[0;32m    257\u001b[0m             )\n\u001b[1;32m--> 259\u001b[0m expl_perturbed_inputs \u001b[38;5;241m=\u001b[39m explanation_func(inputs_perturbed, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs_copy)\n\u001b[0;32m    261\u001b[0m \u001b[38;5;66;03m# tuplize `expl_perturbed_inputs` in case it is not\u001b[39;00m\n\u001b[0;32m    262\u001b[0m expl_perturbed_inputs \u001b[38;5;241m=\u001b[39m _format_tensor_into_tuples(expl_perturbed_inputs)\n",
      "File \u001b[1;32mc:\\Users\\Luca\\Desktop\\MagistraleAI\\2nd_year\\Artificial Intelligence in Industry\\project\\metrics\\sensitivity.py:43\u001b[0m, in \u001b[0;36mSensitivity.__call__.<locals>.attribution_wrapper\u001b[1;34m(images, model, layer, targets, baseline_dist)\u001b[0m\n\u001b[0;32m     39\u001b[0m \u001b[38;5;66;03m# Repeat targets\u001b[39;00m\n\u001b[0;32m     40\u001b[0m batch_targets \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mrepeat_interleave(targets, BATCH_SIZE, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n\u001b[0;32m     42\u001b[0m attribution_res \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m---> 43\u001b[0m     \u001b[43mattribution_method\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mattribute\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     44\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlayer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_targets\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbaseline_dist\u001b[49m\n\u001b[0;32m     45\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     46\u001b[0m     \u001b[38;5;241m.\u001b[39mdetach()\n\u001b[0;32m     47\u001b[0m     \u001b[38;5;241m.\u001b[39mcpu()\n\u001b[0;32m     48\u001b[0m )\n\u001b[0;32m     49\u001b[0m ATTRIBUTION_SHAPE \u001b[38;5;241m=\u001b[39m attribution_res\u001b[38;5;241m.\u001b[39mshape\n\u001b[0;32m     51\u001b[0m \u001b[38;5;66;03m# If any of the attributions is NaN, skip the batch\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Luca\\Desktop\\MagistraleAI\\2nd_year\\Artificial Intelligence in Industry\\project\\utils\\attributions.py:107\u001b[0m, in \u001b[0;36m_DeepLiftShap.attribute\u001b[1;34m(self, input_tensor, model, layer, target, baseline_dist, normalize)\u001b[0m\n\u001b[0;32m    104\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    105\u001b[0m     baseline_dist \u001b[38;5;241m=\u001b[39m model_to(baseline_dist)\n\u001b[1;32m--> 107\u001b[0m attributions, delta \u001b[38;5;241m=\u001b[39m \u001b[43mdl\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mattribute\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    108\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_to_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbaseline_dist\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturn_convergence_delta\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\n\u001b[0;32m    109\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    110\u001b[0m attributions \u001b[38;5;241m=\u001b[39m attributions\u001b[38;5;241m.\u001b[39msum(dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m, keepdim\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m    112\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m normalize:\n",
      "File \u001b[1;32mc:\\Users\\Luca\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\captum\\log\\__init__.py:42\u001b[0m, in \u001b[0;36mlog_usage.<locals>._log_usage.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     40\u001b[0m \u001b[38;5;129m@wraps\u001b[39m(func)\n\u001b[0;32m     41\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrapper\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m---> 42\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\Luca\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\captum\\attr\\_core\\deep_lift.py:788\u001b[0m, in \u001b[0;36mDeepLiftShap.attribute\u001b[1;34m(self, inputs, baselines, target, additional_forward_args, return_convergence_delta, custom_attribution_func)\u001b[0m\n\u001b[0;32m    778\u001b[0m base_bsz \u001b[38;5;241m=\u001b[39m baselines[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m    780\u001b[0m (\n\u001b[0;32m    781\u001b[0m     exp_inp,\n\u001b[0;32m    782\u001b[0m     exp_base,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    786\u001b[0m     baselines, inputs, target, additional_forward_args\n\u001b[0;32m    787\u001b[0m )\n\u001b[1;32m--> 788\u001b[0m attributions \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mattribute\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__wrapped__\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore\u001b[39;49;00m\n\u001b[0;32m    789\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    790\u001b[0m \u001b[43m    \u001b[49m\u001b[43mexp_inp\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    791\u001b[0m \u001b[43m    \u001b[49m\u001b[43mexp_base\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    792\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtarget\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mexp_tgt\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    793\u001b[0m \u001b[43m    \u001b[49m\u001b[43madditional_forward_args\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mexp_addit_args\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    794\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_convergence_delta\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcast\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    795\u001b[0m \u001b[43m        \u001b[49m\u001b[43mLiteral\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturn_convergence_delta\u001b[49m\n\u001b[0;32m    796\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    797\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcustom_attribution_func\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcustom_attribution_func\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    798\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    799\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m return_convergence_delta:\n\u001b[0;32m    800\u001b[0m     attributions, delta \u001b[38;5;241m=\u001b[39m cast(Tuple[Tuple[Tensor, \u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m], Tensor], attributions)\n",
      "File \u001b[1;32mc:\\Users\\Luca\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\captum\\attr\\_core\\deep_lift.py:330\u001b[0m, in \u001b[0;36mDeepLift.attribute\u001b[1;34m(self, inputs, baselines, target, additional_forward_args, return_convergence_delta, custom_attribution_func)\u001b[0m\n\u001b[0;32m    320\u001b[0m expanded_target \u001b[38;5;241m=\u001b[39m _expand_target(\n\u001b[0;32m    321\u001b[0m     target, \u001b[38;5;241m2\u001b[39m, expansion_type\u001b[38;5;241m=\u001b[39mExpansionTypes\u001b[38;5;241m.\u001b[39mrepeat\n\u001b[0;32m    322\u001b[0m )\n\u001b[0;32m    324\u001b[0m wrapped_forward_func \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_construct_forward_func(\n\u001b[0;32m    325\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel,\n\u001b[0;32m    326\u001b[0m     (inputs, baselines),\n\u001b[0;32m    327\u001b[0m     expanded_target,\n\u001b[0;32m    328\u001b[0m     additional_forward_args,\n\u001b[0;32m    329\u001b[0m )\n\u001b[1;32m--> 330\u001b[0m gradients \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgradient_func\u001b[49m\u001b[43m(\u001b[49m\u001b[43mwrapped_forward_func\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    331\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m custom_attribution_func \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    332\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmultiplies_by_inputs:\n",
      "File \u001b[1;32mc:\\Users\\Luca\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\captum\\_utils\\gradient.py:112\u001b[0m, in \u001b[0;36mcompute_gradients\u001b[1;34m(forward_fn, inputs, target_ind, additional_forward_args)\u001b[0m\n\u001b[0;32m     94\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m     95\u001b[0m \u001b[38;5;124;03mComputes gradients of the output with respect to inputs for an\u001b[39;00m\n\u001b[0;32m     96\u001b[0m \u001b[38;5;124;03marbitrary forward function.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    108\u001b[0m \u001b[38;5;124;03m                arguments) if no additional arguments are required\u001b[39;00m\n\u001b[0;32m    109\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    110\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mautograd\u001b[38;5;241m.\u001b[39mset_grad_enabled(\u001b[38;5;28;01mTrue\u001b[39;00m):\n\u001b[0;32m    111\u001b[0m     \u001b[38;5;66;03m# runs forward pass\u001b[39;00m\n\u001b[1;32m--> 112\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[43m_run_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mforward_fn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget_ind\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43madditional_forward_args\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    113\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m outputs[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mnumel() \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m, (\n\u001b[0;32m    114\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTarget not provided when necessary, cannot\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    115\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m take gradient with respect to multiple outputs.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    116\u001b[0m     )\n\u001b[0;32m    117\u001b[0m     \u001b[38;5;66;03m# torch.unbind(forward_out) is a list of scalar tensor tuples and\u001b[39;00m\n\u001b[0;32m    118\u001b[0m     \u001b[38;5;66;03m# contains batch_size * #steps elements\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Luca\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\captum\\_utils\\common.py:523\u001b[0m, in \u001b[0;36m_run_forward\u001b[1;34m(forward_func, inputs, target, additional_forward_args)\u001b[0m\n\u001b[0;32m    521\u001b[0m forward_func_args \u001b[38;5;241m=\u001b[39m signature(forward_func)\u001b[38;5;241m.\u001b[39mparameters\n\u001b[0;32m    522\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(forward_func_args) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m--> 523\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[43mforward_func\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    524\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m output \u001b[38;5;28;01mif\u001b[39;00m target \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m _select_targets(output, target)\n\u001b[0;32m    526\u001b[0m \u001b[38;5;66;03m# make everything a tuple so that it is easy to unpack without\u001b[39;00m\n\u001b[0;32m    527\u001b[0m \u001b[38;5;66;03m# using if-statements\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Luca\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\captum\\attr\\_core\\deep_lift.py:369\u001b[0m, in \u001b[0;36mDeepLift._construct_forward_func.<locals>.forward_fn\u001b[1;34m()\u001b[0m\n\u001b[0;32m    368\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward_fn\u001b[39m():\n\u001b[1;32m--> 369\u001b[0m     model_out \u001b[38;5;241m=\u001b[39m \u001b[43m_run_forward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    370\u001b[0m \u001b[43m        \u001b[49m\u001b[43mforward_func\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43madditional_forward_args\u001b[49m\n\u001b[0;32m    371\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    372\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _select_targets(\n\u001b[0;32m    373\u001b[0m         torch\u001b[38;5;241m.\u001b[39mcat((model_out[:, \u001b[38;5;241m0\u001b[39m], model_out[:, \u001b[38;5;241m1\u001b[39m])), target\n\u001b[0;32m    374\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\Luca\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\captum\\_utils\\common.py:531\u001b[0m, in \u001b[0;36m_run_forward\u001b[1;34m(forward_func, inputs, target, additional_forward_args)\u001b[0m\n\u001b[0;32m    528\u001b[0m inputs \u001b[38;5;241m=\u001b[39m _format_inputs(inputs)\n\u001b[0;32m    529\u001b[0m additional_forward_args \u001b[38;5;241m=\u001b[39m _format_additional_forward_args(additional_forward_args)\n\u001b[1;32m--> 531\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[43mforward_func\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    532\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43madditional_forward_args\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    533\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43madditional_forward_args\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\n\u001b[0;32m    534\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\n\u001b[0;32m    535\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    536\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _select_targets(output, target)\n",
      "File \u001b[1;32mc:\\Users\\Luca\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\Luca\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\module.py:1582\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1579\u001b[0m     bw_hook \u001b[38;5;241m=\u001b[39m hooks\u001b[38;5;241m.\u001b[39mBackwardHook(\u001b[38;5;28mself\u001b[39m, full_backward_hooks, backward_pre_hooks)\n\u001b[0;32m   1580\u001b[0m     args \u001b[38;5;241m=\u001b[39m bw_hook\u001b[38;5;241m.\u001b[39msetup_input_hook(args)\n\u001b[1;32m-> 1582\u001b[0m result \u001b[38;5;241m=\u001b[39m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1583\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks:\n\u001b[0;32m   1584\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m hook_id, hook \u001b[38;5;129;01min\u001b[39;00m (\n\u001b[0;32m   1585\u001b[0m         \u001b[38;5;241m*\u001b[39m_global_forward_hooks\u001b[38;5;241m.\u001b[39mitems(),\n\u001b[0;32m   1586\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks\u001b[38;5;241m.\u001b[39mitems(),\n\u001b[0;32m   1587\u001b[0m     ):\n\u001b[0;32m   1588\u001b[0m         \u001b[38;5;66;03m# mark that always called hook is run\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Luca\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\container.py:217\u001b[0m, in \u001b[0;36mSequential.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    215\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[0;32m    216\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[1;32m--> 217\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    218\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\Luca\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\Luca\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Luca\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\conv.py:460\u001b[0m, in \u001b[0;36mConv2d.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    459\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m--> 460\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_conv_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Luca\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\conv.py:456\u001b[0m, in \u001b[0;36mConv2d._conv_forward\u001b[1;34m(self, input, weight, bias)\u001b[0m\n\u001b[0;32m    452\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mzeros\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[0;32m    453\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m F\u001b[38;5;241m.\u001b[39mconv2d(F\u001b[38;5;241m.\u001b[39mpad(\u001b[38;5;28minput\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reversed_padding_repeated_twice, mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode),\n\u001b[0;32m    454\u001b[0m                     weight, bias, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstride,\n\u001b[0;32m    455\u001b[0m                     _pair(\u001b[38;5;241m0\u001b[39m), \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdilation, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgroups)\n\u001b[1;32m--> 456\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv2d\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    457\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpadding\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdilation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgroups\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from metrics import calculate_metrics, Infidelity, RoadCombined, Sensitivity\n",
    "from utils import ERFUpsamplingFast\n",
    "\n",
    "RESULT_PATH = \"./results.csv\"\n",
    "\n",
    "results = ResultMetrics(RESULT_PATH)\n",
    "\n",
    "for layer in model.features[10:]:\n",
    "    for attribution in [_GradCAMPlusPlus(model, layer), _DeepLiftShap()]:\n",
    "        for upscale in [bilinear_upsampling, ERFUpsamplingFast(model, layer, device)]:\n",
    "            calculate_metrics(model,\n",
    "                            attribution,\n",
    "                            dl_test,\n",
    "                            dl_train,\n",
    "                            layers=[layer],\n",
    "                            metrics=[RoadCombined()],\n",
    "                            result_metrics=results,\n",
    "                            device=device,\n",
    "                            upsample=upscale,\n",
    "                            rescale_saliency=False,\n",
    "                            model_name=\"VGG11\",\n",
    "                            debug=False\n",
    "                            )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DeepLiftShap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results loaded from ./results.csv.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "05252e0298f9480db8fabe73baff7e8a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([2])\n",
      "torch.Size([8, 3, 224, 224])\n",
      "<class 'tuple'>\n",
      "(tensor([[[[ 0.5536,  0.5536,  0.5536,  ...,  1.1187,  0.0741, -0.2513],\n",
      "          [ 0.5878,  0.5878,  0.5707,  ...,  0.8104, -0.2171, -0.5424],\n",
      "          [ 0.6392,  0.6392,  0.5878,  ...,  0.2111, -0.8164, -1.1418],\n",
      "          ...,\n",
      "          [ 0.2111,  0.2111,  0.2624,  ..., -1.4158, -1.4672, -1.5014],\n",
      "          [ 0.1768,  0.1939,  0.2453,  ..., -1.4329, -1.4500, -1.4500],\n",
      "          [ 0.1597,  0.1768,  0.2282,  ..., -1.4500, -1.4329, -1.4329]],\n",
      "\n",
      "         [[ 0.9930,  0.9930,  1.0105,  ...,  1.2031,  0.1352, -0.1975],\n",
      "          [ 1.0280,  1.0280,  1.0280,  ...,  0.8880, -0.1800, -0.5126],\n",
      "          [ 1.1155,  1.0980,  1.0805,  ...,  0.2402, -0.7927, -1.1253],\n",
      "          ...,\n",
      "          [-0.5476, -0.5301, -0.5126,  ..., -1.0553, -1.1429, -1.1604],\n",
      "          [-0.6001, -0.5826, -0.5301,  ..., -1.1078, -1.1253, -1.1429],\n",
      "          [-0.6176, -0.6001, -0.5476,  ..., -1.1429, -1.1253, -1.1253]],\n",
      "\n",
      "         [[ 1.5245,  1.5245,  1.5071,  ...,  0.6705, -0.3230, -0.6367],\n",
      "          [ 1.5594,  1.5594,  1.5420,  ...,  0.3916, -0.6018, -0.9156],\n",
      "          [ 1.6291,  1.6291,  1.5942,  ..., -0.1835, -1.1770, -1.4733],\n",
      "          ...,\n",
      "          [-0.6890, -0.6890, -0.6715,  ..., -1.2641, -1.3164, -1.3339],\n",
      "          [-0.7413, -0.7413, -0.7064,  ..., -1.2816, -1.2990, -1.2990],\n",
      "          [-0.7761, -0.7587, -0.7238,  ..., -1.2990, -1.2816, -1.2816]]]]),)\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'tuple' object has no attribute 'shape'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[13], line 8\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m f \u001b[38;5;129;01min\u001b[39;00m model\u001b[38;5;241m.\u001b[39mfeatures:\n\u001b[0;32m      7\u001b[0m     attr \u001b[38;5;241m=\u001b[39m _GradCAMPlusPlus(model, f)\n\u001b[1;32m----> 8\u001b[0m     \u001b[43mcalculate_metrics\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      9\u001b[0m \u001b[43m                    \u001b[49m\u001b[43mattr\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     10\u001b[0m \u001b[43m                    \u001b[49m\u001b[43mdl_test\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     11\u001b[0m \u001b[43m                    \u001b[49m\u001b[43mdl_train\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     12\u001b[0m \u001b[43m                    \u001b[49m\u001b[43mlayers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43mf\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     13\u001b[0m \u001b[43m                    \u001b[49m\u001b[43mmetrics\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43mSensitivity\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mInfidelity\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mRoadCombined\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     14\u001b[0m \u001b[43m                    \u001b[49m\u001b[43mresult_metrics\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresults\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     15\u001b[0m \u001b[43m                    \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     16\u001b[0m \u001b[43m                    \u001b[49m\u001b[43mupsample\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbilinear_upsampling\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     17\u001b[0m \u001b[43m                    \u001b[49m\u001b[43mrescale_saliency\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m     18\u001b[0m \u001b[43m                    \u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mVGG11\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     19\u001b[0m \u001b[43m                    \u001b[49m\u001b[43mdebug\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\n\u001b[0;32m     20\u001b[0m \u001b[43m                    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Luca\\Desktop\\MagistraleAI\\2nd_year\\Artificial Intelligence in Industry\\project\\metrics\\metric.py:131\u001b[0m, in \u001b[0;36mcalculate_metrics\u001b[1;34m(model, attribute_method, test_dl, train_dl, layers, metrics, result_metrics, upsample, device, rescale_saliency, rescale_perc, model_name, debug)\u001b[0m\n\u001b[0;32m    129\u001b[0m \u001b[38;5;66;03m# Calculate all the metrics\u001b[39;00m\n\u001b[0;32m    130\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m metric \u001b[38;5;129;01min\u001b[39;00m metrics:\n\u001b[1;32m--> 131\u001b[0m     metric_res \u001b[38;5;241m=\u001b[39m \u001b[43mmetric\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    132\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    133\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtest_images\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mimages\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    134\u001b[0m \u001b[43m        \u001b[49m\u001b[43msaliency_maps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msaliency_maps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    135\u001b[0m \u001b[43m        \u001b[49m\u001b[43mclass_idx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlabels\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    136\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattribution_method\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattribute_method\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    137\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    138\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbaseline_dist\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbaseline_dist\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    139\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlayer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlayer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    140\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    142\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m metric_res \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    143\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMetric \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmetric\u001b[38;5;241m.\u001b[39mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m is None\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\Luca\\Desktop\\MagistraleAI\\2nd_year\\Artificial Intelligence in Industry\\project\\metrics\\sensitivity.py:81\u001b[0m, in \u001b[0;36mSensitivity.__call__\u001b[1;34m(self, model, test_images, saliency_maps, class_idx, attribution_method, device, apply_softmax, return_mean, **kwargs)\u001b[0m\n\u001b[0;32m     78\u001b[0m kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m model\n\u001b[0;32m     79\u001b[0m kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtargets\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m class_idx\n\u001b[1;32m---> 81\u001b[0m sens \u001b[38;5;241m=\u001b[39m sensitivity_max(attribution_wrapper, test_images, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m     83\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (sens \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m100\u001b[39m)\u001b[38;5;241m.\u001b[39many():\n\u001b[0;32m     84\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Luca\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\captum\\log\\__init__.py:42\u001b[0m, in \u001b[0;36mlog_usage.<locals>._log_usage.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     40\u001b[0m \u001b[38;5;129m@wraps\u001b[39m(func)\n\u001b[0;32m     41\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrapper\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m---> 42\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\Luca\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\captum\\metrics\\_core\\sensitivity.py:310\u001b[0m, in \u001b[0;36msensitivity_max\u001b[1;34m(explanation_func, inputs, perturb_func, perturb_radius, n_perturb_samples, norm_ord, max_examples_per_batch, **kwargs)\u001b[0m\n\u001b[0;32m    307\u001b[0m bsz \u001b[38;5;241m=\u001b[39m inputs[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m0\u001b[39m)\n\u001b[0;32m    309\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[1;32m--> 310\u001b[0m     expl_inputs \u001b[38;5;241m=\u001b[39m explanation_func(inputs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    311\u001b[0m     metrics_max \u001b[38;5;241m=\u001b[39m _divide_and_aggregate_metrics(\n\u001b[0;32m    312\u001b[0m         cast(Tuple[Tensor, \u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m], inputs),\n\u001b[0;32m    313\u001b[0m         n_perturb_samples,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    316\u001b[0m         agg_func\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mmax,\n\u001b[0;32m    317\u001b[0m     )\n\u001b[0;32m    318\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m metrics_max\n",
      "File \u001b[1;32mc:\\Users\\Luca\\Desktop\\MagistraleAI\\2nd_year\\Artificial Intelligence in Industry\\project\\metrics\\sensitivity.py:33\u001b[0m, in \u001b[0;36mSensitivity.__call__.<locals>.attribution_wrapper\u001b[1;34m(images, model, layer, targets, baseline_dist)\u001b[0m\n\u001b[0;32m     31\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;28mtype\u001b[39m(images))\n\u001b[0;32m     32\u001b[0m \u001b[38;5;28mprint\u001b[39m(images)\n\u001b[1;32m---> 33\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[43mimages\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshape\u001b[49m, targets\u001b[38;5;241m.\u001b[39mshape, baseline_dist\u001b[38;5;241m.\u001b[39mshape)\n\u001b[0;32m     34\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(images) \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28mtuple\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(images) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m     35\u001b[0m     images \u001b[38;5;241m=\u001b[39m images[\u001b[38;5;241m0\u001b[39m]\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'tuple' object has no attribute 'shape'"
     ]
    }
   ],
   "source": [
    "from metrics import calculate_metrics, Infidelity, RoadCombined, Sensitivity\n",
    "\n",
    "RESULT_PATH = \"./results.csv\"\n",
    "\n",
    "results = ResultMetrics(RESULT_PATH)\n",
    "for f in model.features:\n",
    "    attr = _GradCAMPlusPlus(model, f)\n",
    "    calculate_metrics(model,\n",
    "                    attr,\n",
    "                    dl_test,\n",
    "                    dl_train,\n",
    "                    layers=[f],\n",
    "                    metrics=[Sensitivity(), Infidelity(), RoadCombined()],\n",
    "                    result_metrics=results,\n",
    "                    device=device,\n",
    "                    upsample=bilinear_upsampling,\n",
    "                    rescale_saliency=False,\n",
    "                    model_name=\"VGG11\",\n",
    "                    debug=False\n",
    "                    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DeepLiftShap with rescaling ($r=0.3$)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from metrics import calculate_metrics\n",
    "\n",
    "RESULT_PATH = \"./results.csv\"\n",
    "\n",
    "results = ResultMetrics(RESULT_PATH)\n",
    "calculate_metrics(model,\n",
    "                _DeepLiftShap(),\n",
    "                dl_test,\n",
    "                dl_train,\n",
    "                layers=[f for f in model.features],\n",
    "                result_metrics=results,\n",
    "                device=device,\n",
    "                upsample=bilinear_upsampling,\n",
    "                rescale_saliency=True,\n",
    "                model_name=\"VGG11_Rescaling\",\n",
    "                debug=False\n",
    "                )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GradCam++"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from metrics import calculate_metrics\n",
    "\n",
    "RESULT_PATH = \"./results.csv\"\n",
    "\n",
    "results = ResultMetrics(RESULT_PATH)\n",
    "\n",
    "for f in model.features:\n",
    "    attr = _GradCAMPlusPlus(model, f)\n",
    "    calculate_metrics(model,\n",
    "                    attr,\n",
    "                    dl_test,\n",
    "                    dl_train,\n",
    "                    layers=[f],\n",
    "                    result_metrics=results,\n",
    "                    device=device,\n",
    "                    upsample=bilinear_upsampling,\n",
    "                    rescale_saliency=False,\n",
    "                    model_name=\"VGG11\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GradCam++ with rescaling ($r=0.3$)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from metrics import calculate_metrics\n",
    "\n",
    "RESULT_PATH = \"./results.csv\"\n",
    "\n",
    "results = ResultMetrics(RESULT_PATH)\n",
    "\n",
    "for f in model.features:\n",
    "    attr = _GradCAMPlusPlus(model, f)\n",
    "    calculate_metrics(model,\n",
    "                    attr,\n",
    "                    dl_test,\n",
    "                    dl_train,\n",
    "                    layers=[f],\n",
    "                    result_metrics=results,\n",
    "                    device=device,\n",
    "                    upsample=bilinear_upsampling,\n",
    "                    rescale_saliency=True,\n",
    "                    model_name=\"VGG11_Rescaling\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE_TEST = 1\n",
    "NUM_TEST = 8\n",
    "\n",
    "dl_test_small = DataLoader(Subset(test_data, torch.randperm(len(test_data))[:NUM_TEST]), batch_size=BATCH_SIZE_TEST, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results loaded from ./results.csv.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fb251998173849b281dfd0c965f32e7d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Luca\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\captum\\attr\\_core\\deep_lift.py:304: UserWarning: Setting forward, backward hooks and attributes on non-linear\n",
      "               activations. The hooks and attributes will be removed\n",
      "            after the attribution is finished\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fe9c6d9f57f549c0a7ec5f9655461059",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "erf.shape:(7, 7, 3, 224, 224)\n",
      "attribution.shape:(7, 7)\n",
      "result.shape:(3, 224, 224)\n",
      "image.shape:torch.Size([1, 3, 224, 224])\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "529798f4fd8b443b8fd175729cac1a37",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "erf.shape:(7, 7, 3, 224, 224)\n",
      "attribution.shape:(7, 7)\n",
      "result.shape:(3, 224, 224)\n",
      "image.shape:torch.Size([1, 3, 224, 224])\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e0e5b498778041d48e8d598e8e349861",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "erf.shape:(7, 7, 3, 224, 224)\n",
      "attribution.shape:(7, 7)\n",
      "result.shape:(3, 224, 224)\n",
      "image.shape:torch.Size([1, 3, 224, 224])\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e50f066f3a6945968ade70095d37a2fd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "erf.shape:(7, 7, 3, 224, 224)\n",
      "attribution.shape:(7, 7)\n",
      "result.shape:(3, 224, 224)\n",
      "image.shape:torch.Size([1, 3, 224, 224])\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e69cf328a6f14848bac4e2d2deb09d2d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "erf.shape:(7, 7, 3, 224, 224)\n",
      "attribution.shape:(7, 7)\n",
      "result.shape:(3, 224, 224)\n",
      "image.shape:torch.Size([1, 3, 224, 224])\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "70ba28717c3b412b8c59c2760046cfae",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "erf.shape:(7, 7, 3, 224, 224)\n",
      "attribution.shape:(7, 7)\n",
      "result.shape:(3, 224, 224)\n",
      "image.shape:torch.Size([1, 3, 224, 224])\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f8244896b4ea4639b0a2d9b6fff1acbd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "erf.shape:(7, 7, 3, 224, 224)\n",
      "attribution.shape:(7, 7)\n",
      "result.shape:(3, 224, 224)\n",
      "image.shape:torch.Size([1, 3, 224, 224])\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9480361c3acc4618ba9654f47825383f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "erf.shape:(7, 7, 3, 224, 224)\n",
      "attribution.shape:(7, 7)\n",
      "result.shape:(3, 224, 224)\n",
      "image.shape:torch.Size([1, 3, 224, 224])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Luca\\Desktop\\MagistraleAI\\2nd_year\\Artificial Intelligence in Industry\\project\\results\\results_metrics.py:76: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  self.results = pd.concat(\n"
     ]
    }
   ],
   "source": [
    "from metrics import calculate_metrics\n",
    "\n",
    "RESULT_PATH = \"./results.csv\"\n",
    "\n",
    "results = ResultMetrics(RESULT_PATH)\n",
    "for layer in model.features[-1:]:\n",
    "    feature_extractor = cut_model_to_layer(model, layer, included=True)  # Model from the start up to the layer\n",
    "    erf_upsampling = ERFUpsampling(feature_extractor, device)\n",
    "    calculate_metrics(model,\n",
    "                    _DeepLiftShap(),\n",
    "                    dl_test_small,\n",
    "                    dl_train,\n",
    "                    layers=[layer],\n",
    "                    result_metrics=results,\n",
    "                    device=device,\n",
    "                    upsample=erf_upsampling,\n",
    "                    rescale_saliency=False,\n",
    "                    model_name=\"VGG11\",\n",
    "                    debug=False\n",
    "                    )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
